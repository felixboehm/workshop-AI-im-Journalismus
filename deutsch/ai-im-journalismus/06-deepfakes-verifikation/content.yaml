version: 1
number: 6
title: "Deepfakes und Verifikation"
description: "Erkennung manipulierter Medien und Werkzeuge zur Verifikation"
sections:
  - title: "Deepfakes verstehen"
    explanation: |
      **Deepfakes** sind KI-generierte oder manipulierte Medien (Bilder, Videos, Audio),
      die taueschend echt wirken. Die Zahlen sind alarmierend:

      - Rund **500.000 Deepfake-Videos** wurden 2023 in sozialen Medien geteilt
      - Prognosen zeigen **bis zu 8 Millionen Deepfakes bis 2025**
      - Die russisch-ausgerichtete Kampagne "Operation Overload" imitierte 2025 ueber 80 Organisationen mittels manipulierter Logos und Stimmen
      - Deepfake-Technologie wird zunehmend fuer Desinformationskampagnen eingesetzt

      Fuer Journalist:innen bedeutet das: Jedes Bild, Video oder Audio muss potenziell
      verifiziert werden, bevor es veroeffentlicht wird.
    examples:
      - q: "Ein Video zeigt angeblich einen Politiker bei einer kompromittierenden Aussage. Wie geht ein Journalist damit um, bevor er es veroeffentlicht?"
        a: "Erstens: Nicht unter Zeitdruck veroeffentlichen. Zweitens: Wurde das Video auch von anderen Quellen aufgenommen? Gibt es eine unabhaengige Aufnahme? Drittens: Metadaten pruefen (Aufnahmedatum, Geraet, GPS). Viertens: Deepfake-Scanner wie InVID/WeVerify oder Deepware Scanner nutzen. Fuenftens: Auf visuelle Artefakte pruefen (Lippensynchronisation, Augenbewegungen). Bei Zweifeln forensische Expert:innen einbeziehen"
        rel:
          - ["Verifikationspflicht", "Schrittweises Vorgehen bei verdaechtigen Medien"]

      - q: "Operation Overload imitierte 2025 ueber 80 Organisationen. Wie konnten professionelle Nachrichtenredaktionen darauf hereinfallen?"
        a: "Die Kampagne nutzte KI-generierte Stimmen und manipulierte Logos, die auf den ersten Blick authentisch wirkten. Der Trick: Die Desinformation wurde in grosser Menge verbreitet, um Redaktionen zu ueberlasten ('Overload'). Wenn dutzende scheinbar verschiedene Quellen dieselbe Behauptung verbreiten, steigt die Versuchung, sie fuer glaubwuerdig zu halten -- klassisches Social Engineering gegen journalistische Workflows"
        rel:
          - ["Desinformationskampagnen", "Ueberflutungstaktik gegen Redaktionsworkflows"]

      - q: "Deepfake-Technologie wird zunehmend fuer geschlechtsspezifische Gewalt eingesetzt. Was bedeutet das fuer den Journalismus?"
        a: "Zwei Dimensionen: Erstens, Journalistinnen werden selbst Ziel von Deepfake-Attacken, was ihre Arbeit und demokratische Teilhabe gefaehrdet. Zweitens, Redaktionen muessen bei der Berichterstattung ueber Deepfake-Missbrauch besonders sensibel vorgehen und duerfen das Material nicht weiterverbreiten. Die rasante Zunahme von 500.000 auf prognostizierte 8 Millionen Deepfakes zeigt die Dringlichkeit"
        rel:
          - ["Deepfakes und Gewalt", "Missbrauch der Technologie gegen Journalistinnen und Frauen"]

      - q: "Ein Leser schickt ein Smartphone-Video eines Unfalls. Es ist kein Deepfake, aber moeglicherweise aus einem anderen Kontext. Wie prueft man das?"
        a: "Kontextmanipulation ist haeufiger als Deepfakes. Reverse Image Search (Google, TinEye, Yandex) zeigt, ob das Bild schon frueher im Netz auftauchte. Geoverifikation prueft den Aufnahmeort anhand visueller Hinweise (Strassenschilder, Gebaeude, Vegetation). EXIF-Daten zeigen Aufnahmedatum und -geraet. Oft sind es authentische Videos, die zeitlich oder oertlich falsch eingeordnet werden"
        rel:
          - ["Kontextmanipulation", "Authentische Medien in falschem Zusammenhang"]

  - title: "Erkennungsmethoden und Tools"
    explanation: |
      Journalist:innen haben verschiedene Werkzeuge zur Deepfake-Erkennung:

      - **InVID/WeVerify**: Video-Verifikation und Metadatenanalyse, speziell fuer Journalist:innen
      - **Hive Moderation**: KI-generierte Bild- und Videoerkennung
      - **Deepware Scanner**: Schnelles Erstscreening von Videos
      - **Content Credentials (C2PA)**: Provenienz-Standard fuer digitale Medien
      - **DeepFake-o-Meter**: Akademisches Erkennungstool fuer vertiefende Analyse

      **Erkennungsmethoden:**
      - Visuelle Forensik: Gesichtsinkonsistenzen, unnatuerliche Augenbewegungen, Lippensynchronisations-Fehler
      - Audio-Analyse: Stimmton-Variationen, unnatuerliche Muster
      - Cross-modale Konsistenzpruefung: Abgleich zwischen Audio und Video
    examples:
      - q: "InVID/WeVerify wurde speziell fuer Journalist:innen entwickelt. Was kann es, was eine normale Google-Bildersuche nicht kann?"
        a: "InVID zerlegt Videos in Einzelframes und prueft jeden separat auf Manipulationen, analysiert Metadaten und bietet Reverse-Suche fuer Videoausschnitte. Eine Google-Bildersuche findet nur identische oder aehnliche Bilder. InVID erkennt auch, ob ein Video geschnitten, zeitlich veraendert oder aus dem Kontext gerissen wurde -- das ist bei User Generated Content aus Krisengebieten entscheidend"
        rel:
          - ["InVID/WeVerify", "Spezialisierte Video-Verifikation fuer Journalismus"]

      - q: "Content Credentials (C2PA) sollen die Herkunft digitaler Medien nachvollziehbar machen. Warum ist das besser als Wasserzeichen?"
        a: "Wasserzeichen koennen entfernt oder manipuliert werden. C2PA ist ein kryptografischer Provenienz-Standard: Er dokumentiert lueckenlos, wer ein Medium erstellt hat, ob und wie es bearbeitet wurde und ob KI beteiligt war. Wenn ein Foto C2PA-Credentials hat, kann man die gesamte Bearbeitungskette nachvollziehen. Das Problem: Noch nutzen zu wenige Plattformen und Kameras den Standard"
        rel:
          - ["C2PA", "Kryptografische Herkunftsnachweise statt entfernbarer Wasserzeichen"]

      - q: "Eine Studie zeigte, dass Journalist:innen sich manchmal zu stark auf Deepfake-Erkennungstools verlassen. Wann wird das gefaehrlich?"
        a: "Wenn das Tool die eigene Vermutung bestaetigt -- klassischer Confirmation Bias. Beispiel: Ein Journalist vermutet, ein Video sei echt, und das Tool zeigt 'wahrscheinlich authentisch'. Er veroeffentlicht, ohne weiter zu pruefen. Aber kein Erkennungstool ist 100% zuverlaessig. Die Tools sind Hilfsmittel im Verifikations-Workflow, nicht das Endergebnis. Im Zweifel gilt: Nicht veroeffentlichen"
        rel:
          - ["Confirmation Bias", "Uebermaessiges Vertrauen in bestaetigende Toolergebnisse"]

      - q: "Welche visuellen Merkmale verraten ein KI-generiertes Bild, die ein Laie uebersehen wuerde?"
        a: "Haende mit falscher Fingerzahl oder unnatuerlichen Gelenken, verzerrter Text auf Schildern oder Kleidung, inkonsistente Schatten, zu perfekte Hautstruktur, verschwommene Ohren oder Zaehne, Asymmetrien im Hintergrund. Bei Videos: Lippensynchronisations-Fehler und unnatuerliche Augenbewegungen. Aber: Die Technologie verbessert sich rasant, visuelle Forensik allein wird langfristig nicht reichen"
        rel:
          - ["Visuelle Forensik", "Erkennbare Artefakte in KI-generierten Bildern"]

  - title: "Verifikations-Workflow"
    explanation: |
      Ein empfohlener 6-Schritte-Workflow fuer die Verifikation:

      1. **Erstscreening**: Automatisierte Analyse mit Detektionstools (z.B. Deepware Scanner)
      2. **Metadaten-Pruefung**: EXIF-Daten, Dateieigenschaften, C2PA-Credentials pruefen
      3. **Visuelle Inspektion**: Manuelle Pruefung auf Artefakte und Inkonsistenzen
      4. **Quellenverifikation**: Rueckverfolgung zum Ursprung, Reverse Image Search, Gegenrecherche
      5. **Expert:innen-Konsultation**: Bei Zweifeln forensische Expert:innen einbeziehen
      6. **Redaktionelle Entscheidung**: Menschliche Reviewer:innen bestaetigen Befunde

      Klassische Methoden wie **Reverse Image Search** (Google, TinEye, Yandex) und
      **Geoverifikation** bleiben unverzichtbar, besonders bei User Generated Content.
    examples:
      - q: "Waehrend einer Naturkatastrophe fluten Dutzende Videos die sozialen Medien. Wie priorisiert man die Verifikation unter Zeitdruck?"
        a: "Erstscreening mit automatisierten Tools (Deepware Scanner, InVID) fuer alle Videos gleichzeitig -- das spart Zeit. Dann: Videos von verifizierten Accounts oder Nachrichtenagenturen priorisieren. Reverse Image Search fuer die auffaelligsten Aufnahmen. Geoverifikation fuer standortgebundene Behauptungen. Im Zweifel lieber ein Video weniger veroeffentlichen als ein falsches. Geschwindigkeit darf Verifikation nie ersetzen"
        rel:
          - ["Krisenjournalismus", "Verifikation unter Zeitdruck bei Breaking News"]

      - q: "Warum beginnt der Workflow mit automatisierten Tools und nicht mit manueller Inspektion?"
        a: "Weil automatisierte Tools offensichtliche Manipulationen in Sekunden erkennen koennen, waehrend manuelle Inspektion pro Video Minuten dauert. Deepware Scanner liefert eine Ersteinschaetzung, die den weiteren Aufwand steuert: Bei 'wahrscheinlich manipuliert' folgt sofort intensive Analyse, bei 'wahrscheinlich authentisch' genuegen Metadaten- und Quellenpruefung. Das skaliert bei Dutzenden eingehenden Videos"
        rel:
          - ["Skalierbare Verifikation", "Automatisierte Vorselektion spart menschliche Ressourcen"]

      - q: "Was kann Geoverifikation leisten, was kein automatisiertes Tool schafft?"
        a: "Geoverifikation prueft den realen Kontext: Stimmt der behauptete Aufnahmeort? Passen Strassenschilder, Vegetation, Gebaeude und Lichtverhaeltnisse zusammen? Ein authentisches Video aus einer anderen Stadt oder einem anderen Jahr zu verwenden ist eine haeufige Manipulationsform, die kein Deepfake-Scanner erkennt. Google Maps, Street View und lokale Medienarchive sind hier unverzichtbare Werkzeuge"
        rel:
          - ["Geoverifikation", "Ortspruefung als Ergaenzung zu technischer Analyse"]

      - q: "Warum endet der Workflow mit einer redaktionellen Entscheidung und nicht mit dem Ergebnis der Tools?"
        a: "Weil kein Tool eine 100%-Garantie geben kann. Die redaktionelle Entscheidung wuerdigt den Gesamtkontext: Wie vertrauenswuerdig ist die Quelle? Passt das Material zur bekannten Faktenlage? Welche Konsequenzen haette eine Falschmeldung? Bei einem Video, das Gewalt zeigt, koennte die Entscheidung auch lauten: Authentisch, aber aus ethischen Gruenden nicht veroeffentlichen. Tools liefern Daten, Menschen treffen Entscheidungen"
        rel:
          - ["Redaktionelle Entscheidung", "Menschliches Urteil als finaler Schritt"]
