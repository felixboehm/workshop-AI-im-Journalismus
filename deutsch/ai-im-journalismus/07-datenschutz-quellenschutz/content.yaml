version: 1
number: 7
title: "Datenschutz und Quellenschutz"
description: "Vertraulichkeit und Datensicherheit beim Einsatz von KI-Tools"
sections:
  - title: "Risiken bei Cloud-basierten KI-Tools"
    explanation: |
      Die Nutzung von KI-Tools birgt erhebliche Risiken fuer den Quellenschutz:

      - **Datenlecks**: ChatGPT hatte Sicherheitsvorfaelle, bei denen Gespraechstitel anderer Nutzer angezeigt wurden
      - **Trainingsdaten**: Eingaben koennen verarbeitet, gespeichert und fuer kuenftiges Modelltraining verwendet werden
      - **Vertraulichkeit**: Vertrauliche Quelleninformationen koennten in kuenftige KI-Modelle einfliessen
      - **Verschluesselung**: OpenAI arbeitet an clientseitiger Verschluesselung, deutet aber gleichzeitig automatisierte Ueberwachungssysteme an

      Fuer Journalist:innen, die mit sensiblen Quelleninformationen arbeiten, koennen diese
      Risiken existenziell sein -- ein Datenleck kann Quellen gefaehrden.
    examples:
      - q: "Eine Journalistin recherchiert zu organisierter Kriminalitaet und moechte KI zur Analyse von Gespraecsprotokollen nutzen. Was riskiert sie?"
        a: "Im schlimmsten Fall: das Leben ihrer Quelle. Cloud-basierte KI-Tools speichern Eingaben und koennen sie fuer Modelltraining verwenden. Bei ChatGPT gab es bereits einen Vorfall, bei dem Gespraechstitel anderer Nutzer sichtbar waren. Wenn ein Gespraechsprotokoll mit Klarnamen in einem Cloud-Dienst landet, ist die Vertraulichkeit unwiderruflich gebrochen -- auch wenn das Datenleck nie oeffentlich wird"
        rel:
          - ["Quellengefaehrdung", "Cloud-KI-Tools und vertrauliche Recherche"]

      - q: "OpenAI bietet 'Temporary Chat' und arbeitet an Verschluesselung. Reicht das fuer sensible journalistische Arbeit?"
        a: "Nein. Temporary Chat speichert Gespraeche nicht im Verlauf, aber OpenAI behaelt die Daten bis zu 30 Tage. Gleichzeitig deutet OpenAI automatisierte Scanning-Systeme an, die Verschluesselung untergraben. Fuer wirklich sensible Recherchen gibt es nur eine sichere Loesung: Lokale KI-Modelle wie Ollama/Llama, bei denen die Daten den eigenen Rechner nie verlassen"
        rel:
          - ["Grenzen der Cloud-Sicherheit", "Auch verbesserte Datenschutzfunktionen reichen nicht immer"]

      - q: "Der Spiegel nutzt KI-Tools in der Produktion. Wie kann ein grosses Medienhaus KI einsetzen, ohne den Quellenschutz zu gefaehrden?"
        a: "Durch klare Trennung: KI fuer oeffentlich zugaengliche Informationen (Uebersetzungen, Zusammenfassungen publizierter Berichte) ist unbedenklich. Fuer investigative Recherche mit vertraulichen Quellen: Lokale Modelle oder DSGVO-konforme Tools mit No-Training-Policy wie Trint. Der Spiegel hat KI-Richtlinien formuliert -- das zeigt, dass Nutzung und Schutz vereinbar sind, wenn die Regeln klar definiert sind"
        rel:
          - ["KI mit Regeln", "Trennung von unbedenklicher und sensibler Nutzung"]

      - q: "Warum sind Datenschutzerklaerungen der KI-Anbieter kein ausreichender Schutz?"
        a: "Studien zeigen erhebliche Luecken zwischen dem, was Nutzer glauben, und dem, was tatsaechlich mit ihren Daten passiert. Datenschutzerklaerungen sind lang, juristisch formuliert und aendern sich regelmaessig. Der ChatGPT-Vorfall mit sichtbaren Gespraechstiteln anderer Nutzer war kein Verstoss gegen die Datenschutzerklaerung -- solche Sicherheitsvorfaelle sind dort oft als Moeglichkeit erwaehnt. Fuer Journalist:innen gilt: Im Zweifel keine sensiblen Daten eingeben"
        rel:
          - ["Datenschutz-Illusion", "Luecken zwischen Versprechen und Realitaet"]

  - title: "Praktische Schutzmassnahmen"
    explanation: |
      Journalist:innen koennen sich mit konkreten Massnahmen schuetzen:

      1. **Keine vertraulichen Daten in KI-Tools**: Insbesondere nicht in cloudbasierte Dienste wie ChatGPT, Copilot oder Gemini
      2. **Temporary Chat nutzen**: ChatGPTs temporaerer Chat speichert Gespraeche nicht und nutzt sie nicht fuer Training
      3. **Lokale KI-Modelle**: Fuer sensible Recherchen koennen lokal laufende Modelle (z.B. Ollama/Llama) eine sicherere Alternative sein
      4. **DSGVO-konforme Tools**: Europaeische Tools wie Trint bieten EU-Datenspeicherung und No-Training-Policy
      5. **Redaktionelle Richtlinien**: Klare interne Regeln, welche Informationen in KI-Tools eingegeben werden duerfen

      Das Konzept definiert: **Quellenschutz geht vor Effizienz.**
    examples:
      - q: "Ein Volontaer fragt: 'Kann ich nicht einfach die Namen meiner Quellen durch Pseudonyme ersetzen, bevor ich den Text in ChatGPT einfuege?'"
        a: "Das reduziert das Risiko, eliminiert es aber nicht. KI-Modelle koennen aus Kontextinformationen (Ortsangaben, Zeitraeume, Berufsbezeichnungen, beschriebene Ereignisse) Rueckschluesse auf Personen ziehen -- besonders bei kleinen Gemeinden oder spezifischen Vorfaellen. Ausserdem: Wer garantiert, dass beim naechsten Mal das Pseudonymisieren nicht vergessen wird? Besser ist eine klare Regel: Sensibles Material gehoert nicht in Cloud-Dienste"
        rel:
          - ["Pseudonymisierung", "Nicht ausreichend fuer echten Quellenschutz"]

      - q: "Eine Redaktion will Ollama/Llama lokal nutzen. Was sind die praktischen Huerden?"
        a: "Lokale Modelle brauchen leistungsstarke Hardware (GPU mit ausreichend RAM), technische Einrichtung und regelmaessige Updates. Die Qualitaet ist oft geringer als bei Cloud-Diensten wie ChatGPT oder Claude. Fuer die meisten Aufgaben (Uebersetzung oeffentlicher Texte, Zusammenfassungen) sind Cloud-Tools voellig ausreichend. Lokale Modelle lohnen sich nur fuer den spezifischen Anwendungsfall: sensible Recherche mit vertraulichem Material"
        rel:
          - ["Lokale KI-Modelle", "Hoeherer Aufwand, aber maximaler Datenschutz"]

      - q: "Die AFP nutzt Trint mit EU-Datenspeicherung und No-Training-Policy. Warum ist das fuer europaeische Redaktionen wichtig?"
        a: "DSGVO-Konformitaet bedeutet nicht nur Rechtsschutz, sondern auch Quellenschutz. Wenn Trint Daten in der EU speichert und nicht zum Training verwendet, bleiben Transkriptionen von Interviews, Pressekonferenzen und Hintergrundgespraechen unter europaeischem Datenschutzrecht. Bei US-basierten Diensten ohne solche Garantien koennte ein US-Gerichtsbeschluss Zugang zu den Daten erzwingen"
        rel:
          - ["DSGVO als Schutzschild", "Europaeische Datenspeicherung fuer den Quellenschutz"]

      - q: "Quellenschutz geht vor Effizienz -- aber was, wenn eine schnelle KI-Analyse eine Recherche um Wochen beschleunigen koennte?"
        a: "Die Frage ist nicht ob KI genutzt wird, sondern wie. Oeffentlich zugaengliche Teile der Recherche koennen in Cloud-KI-Tools verarbeitet werden. Vertrauliche Kerndokumente werden lokal analysiert oder manuell bearbeitet. Die NYT zeigt den Weg: Sie nutzt KI intensiv fuer die Analyse grosser Datenmengen, aber nie fuer vertrauliche Quellen. Effizienz und Quellenschutz schliessen sich nicht aus -- sie erfordern einen differenzierten Workflow"
        rel:
          - ["Differenzierter Workflow", "KI-Nutzung nach Sensibilitaetsstufe der Daten"]

  - title: "Vermeidung sensibler Dateneingaben"
    explanation: |
      Redaktionen brauchen klare Regeln, was in KI-Tools eingegeben werden darf:

      **Nie in externe KI-Tools eingeben:**
      - Namen und Kontaktdaten vertraulicher Quellen
      - Unveroeffentiches Material und exklusive Rechercheergebnisse
      - Interne Redaktionsdokumente und Strategiepapiere
      - Personenbezogene Daten (DSGVO!)
      - Material aus laufenden Gerichtsverfahren

      **Sicher nutzbar:**
      - Oeffentlich zugaengliche Informationen
      - Bereits veroeffentlichte Artikel zur Zusammenfassung
      - Allgemeine Recherchefragen ohne sensiblen Kontext
      - Uebersetzung von oeffentlichen Pressemitteilungen
    examples:
      - q: "Ein Gerichtsreporter moechte eine Anklageschrift mit KI zusammenfassen lassen. Darf er das?"
        a: "Kommt darauf an: Ist die Anklageschrift oeffentlich (z.B. in einer oeffentlichen Verhandlung verlesen)? Dann ja, mit einem vertrauenswuerdigen Tool. Ist sie vertraulich oder unter Geheimhaltung? Dann absolut nicht -- Material aus laufenden Gerichtsverfahren gehoert nicht in externe KI-Tools. Im Zweifel: die Rechtsabteilung fragen und das Dokument lieber manuell zusammenfassen"
        rel:
          - ["Gerichtsmaterial", "Vertraulichkeit bei juristischen Dokumenten pruefen"]

      - q: "Eine Redakteurin uebersetzt eine oeffentliche Pressemitteilung der EU-Kommission mit DeepL. Dann moechte sie auch ein geleaktes internes EU-Papier uebersetzen. Wo liegt die Grenze?"
        a: "Die Pressemitteilung ist oeffentlich -- DeepL ist unbedenklich. Das geleakte Papier ist unveroeffentiches Material, dessen Quelle geschuetzt werden muss. DeepL ist ein Cloud-Dienst; das Dokument wuerde auf externe Server uebertragen. Fuer das Leak: Entweder manuell uebersetzen oder ein lokales Uebersetzungsmodell nutzen. Die Grenze ist klar: oeffentlich ja, vertraulich nein"
        rel:
          - ["Grenzziehung", "Oeffentliches Material vs. vertrauliche Dokumente"]

      - q: "Wie ueberzeugt man Redaktionsmitglieder, die Datenschutzregeln fuer KI-Tools als uebertrieben empfinden?"
        a: "Mit konkreten Beispielen: Der ChatGPT-Vorfall, bei dem Gespraechstitel anderer Nutzer sichtbar waren. Der Fall Samsung, wo Mitarbeiter vertraulichen Code in ChatGPT einfuegten, der dann in andere Antworten einfloss. Fuer Journalisten: 'Stell dir vor, der Name deiner vertraulichen Quelle taucht in der ChatGPT-Antwort eines anderen Nutzers auf.' Das macht das Risiko greifbar"
        rel:
          - ["Sensibilisierung", "Konkrete Beispiele statt abstrakter Datenschutzregeln"]

      - q: "Eine Redaktion hat klare Regeln, aber einzelne Journalist:innen umgehen sie aus Bequemlichkeit. Wie setzt man die Einhaltung durch?"
        a: "Nicht durch Ueberwachung, sondern durch drei Massnahmen: Erstens, bequeme sichere Alternativen bereitstellen (z.B. Ollama auf Redaktionsrechnern vorinstallieren). Zweitens, regelmaessige Schulungen mit realen Fallbeispielen. Drittens, KI-Nutzung in Audits einbeziehen -- nicht um zu bestrafen, sondern um Luecken zu erkennen. Die AP verhandelte 90-Tage-Vorankuendigungen bei neuen KI-Tools -- das gibt allen Zeit, sich vorzubereiten"
        rel:
          - ["Richtlinien durchsetzen", "Sichere Alternativen statt Verbote"]
