version: 1
number: 9
title: "Qualitaetskontrolle und Audits"
description: "Systematische Qualitaetssicherung fuer KI-gestuetzten Journalismus"
sections:
  - title: "Das Human-in-the-Loop-Prinzip"
    explanation: |
      Der Grundsatz lautet: **Journalismus ist eine menschengetriebene Taetigkeit.**
      KI kann unterstuetzen, aber nicht das Urteilsvermoegen, die Kreativitaet und die
      Verantwortlichkeit ersetzen, die Qualitaetsjournalismus ausmachen.

      **Umsetzung in der Praxis:**
      - Jeder KI-generierte Inhalt wird vor Veroeffentlichung von menschlichen Redakteur:innen geprueft
      - Vier-Augen-Prinzip bei KI-generierten Inhalten
      - KI-Output wird wie Agenturmaterial behandelt: Rohmaterial, das bearbeitet werden muss
      - Menschliche Redakteur:innen haben immer das letzte Wort

      Rund **90% der untersuchten Richtlinien** (aus einer Analyse von 45 Nachrichtenorganisationen)
      schreiben vor, dass KI-Einsatz in einer Story offengelegt werden muss.
    examples:
      - q: "Die AP verbietet KI fuer publishable Content, die BBC erlaubt KI mit Aufsicht, Reuters sagt 'a story is a story'. Welcher Ansatz ist der richtige?"
        a: "Alle drei sind konsistent mit Human-in-the-Loop, aber auf verschiedenen Stufen. Die AP setzt die strengste Grenze: KI nur intern. Die BBC erlaubt KI-Output mit redaktioneller Begruendung und Aufsicht. Reuters fokussiert auf das Ergebnis: Egal wie ein Artikel entsteht, er muss Reuters-Standards entsprechen. Fuer eine Redaktion haengt die Wahl vom Risikoprofil ab -- eine Nachrichtenagentur traegt mehr Verantwortung als ein Nischenmagazin"
        rel:
          - ["Human-in-the-Loop-Varianten", "Verschiedene Umsetzungen desselben Prinzips"]

      - q: "90% der Richtlinien fordern Offenlegung des KI-Einsatzes. Aber wie offenlegt man sinnvoll, ohne den Leser mit technischen Details zu verwirren?"
        a: "Die CBC verfolgt das Prinzip 'keine Ueberraschungen': Der Leser soll nachvollziehen koennen, wie ein Beitrag entstanden ist, ohne jedes Tool benannt zu bekommen. Beispiel: 'Fuer diesen Artikel wurde KI zur Analyse von 500 Kommunaldokumenten eingesetzt. Alle Fakten wurden redaktionell geprueft.' Das ist informativ, aber nicht technisch. Der Guardian und die BBC nutzen aehnlich knappe Hinweise"
        rel:
          - ["Transparenz in der Praxis", "Leserverstaendliche Offenlegung des KI-Einsatzes"]

      - q: "Ein Nachrichtenportal veroeffentlicht taeglich 200 Artikel. Wie realistisch ist Human-in-the-Loop bei diesem Volumen?"
        a: "Es kommt auf die Differenzierung an: Nicht jeder Artikel braucht gleich intensive Pruefung. Datenbasierte Kurzmeldungen (Wetter, Boerse) brauchen stichprobenartige Kontrolle. Nachrichtenartikel mit KI-Unterstuetzung brauchen volle redaktionelle Pruefung. Meinungsstuecke und investigative Berichte sollten nie KI-generiert sein. Das RADAR-Modell zeigt: 8.000 Artikel funktionieren, weil Templates menschlich erstellt und nur Daten maschinell eingefuegt werden"
        rel:
          - ["Skaliertes Human-in-the-Loop", "Differenzierte Prueftiefe nach Artikeltyp"]

      - q: "Was passiert, wenn ein Redakteur KI-Output freigegeben hat und spaeter ein Fehler entdeckt wird? Wer traegt die Verantwortung?"
        a: "Der Redakteur -- und die Redaktion. Die Praeambel-Ergaenzung des deutschen Pressekodex stellt klar: Redaktionen tragen die presseethische Verantwortung fuer alle Beitraege, unabhaengig von der Art der Erstellung. Das ist der Kern von Human-in-the-Loop: Nicht die KI ist verantwortlich, sondern der Mensch, der den Output freigegeben hat. Genau deshalb ist die Pruefung keine Formalitaet, sondern eine echte Verantwortungsuebernahme"
        rel:
          - ["Redaktionelle Verantwortung", "Der Mensch traegt die Verantwortung, nicht die KI"]

  - title: "Qualitaetssicherungs-Checkliste"
    explanation: |
      Jeder KI-gestuetzte Inhalt sollte diese Pruefschritte durchlaufen:

      1. **Faktencheck**: Alle KI-generierten Aussagen gegen Originalquellen pruefen
      2. **Quellenverifikation**: Sind die genannten Quellen real und korrekt zitiert?
      3. **Bias-Pruefung**: Einseitige Darstellungen oder systematische Verzerrungen identifizieren
      4. **Tonalitaet**: Entspricht der Stil den redaktionellen Standards?
      5. **Kennzeichnung**: KI-Einsatz gemaess internen Richtlinien offenlegen
      6. **Versionierung**: Dokumentation der verwendeten Prompts und KI-Tools

      Qualitaetssicherung wird mit KI nicht weniger wichtig, sondern **wichtiger**.
    examples:
      - q: "Ein KI-generierter Artikel nennt eine Studie der 'Universitaet Heidelberg zu Luftverschmutzung 2024'. Wie prueft man, ob diese Studie existiert?"
        a: "Erstens: Direkt auf der Website der Universitaet Heidelberg suchen. Zweitens: In Fachdatenbanken (PubMed, Google Scholar) nach Titel und Autoren suchen. Drittens: Die Pressestelle der Universitaet kontaktieren. KI erfindet regelmaessig Studien mit plausiblen Titeln, Autorennamen und sogar DOI-Nummern. Eine Quelle, die sich nicht in 5 Minuten verifizieren laesst, sollte nicht verwendet werden"
        rel:
          - ["Quellenverifikation", "Konkrete Schritte zur Pruefung KI-generierter Quellen"]

      - q: "Wie erkennt man Bias in einem KI-generierten Artikel ueber Migration, der auf den ersten Blick neutral wirkt?"
        a: "Drei Checks: Erstens, welche Perspektiven kommen vor? Wenn nur Behoerden und Statistiken zitiert werden, fehlen Betroffenenstimmen. Zweitens, welche Wortwahl wird verwendet? 'Migrationsstrom' ist anders konnotiert als 'Zuwanderung'. Drittens, denselben Prompt an ein anderes KI-Modell geben und Unterschiede analysieren. KI-Bias ist oft unsichtbar, weil er die gesellschaftlichen Verzerrungen der Trainingsdaten spiegelt"
        rel:
          - ["Bias in der Praxis", "Konkrete Pruefmethoden fuer unsichtbare Verzerrungen"]

      - q: "Warum ist Versionierung (Dokumentation der Prompts) wichtig, wenn der Artikel ohnehin redaktionell geprueft wird?"
        a: "Drei Gruende: Erstens, wenn spaeter ein Fehler entdeckt wird, kann die Ursache identifiziert werden -- lag es am Prompt, am Modell oder an der Pruefung? Zweitens, erfolgreiche Prompts koennen wiederverwendet und im Team geteilt werden. Drittens, bei externen Beschwerden kann die Redaktion nachweisen, wie der Inhalt entstanden ist. Die NYT dokumentiert KI-Nutzung systematisch -- das schuetzt im Ernstfall"
        rel:
          - ["Prompt-Dokumentation", "Nachvollziehbarkeit als Schutz und Lerngrundlage"]

      - q: "Ein Artikel ist KI-gestuetzt entstanden, aber der Redakteur hat ihn so stark bearbeitet, dass kaum noch Originaltext uebrig ist. Muss der KI-Einsatz trotzdem gekennzeichnet werden?"
        a: "Laut der Mehrheit der Richtlinien: ja. 90% der analysierten Richtlinien fordern Offenlegung des KI-Einsatzes in einer Story, unabhaengig vom Umfang der Bearbeitung. Der EU AI Act fordert ab 2026 ebenfalls Transparenz. Die Frage ist nicht, wie viel KI-Text uebrig ist, sondern ob KI im Entstehungsprozess eingesetzt wurde. Transparenz schuetzt das Vertrauen der Leser"
        rel:
          - ["Kennzeichnungspflicht", "Transparenz unabhaengig vom Bearbeitungsumfang"]

  - title: "KI-Audits in Redaktionen"
    explanation: |
      Nur ca. **50% der Redaktionen** arbeiten an KI-Nutzungsrichtlinien -- systematische
      Audits sind die Ausnahme. Eine Audit-Checkliste fuer Redaktionen:

      1. **Tool-Inventar**: Welche KI-Tools werden eingesetzt?
      2. **Datenfluss**: Welche Daten werden an KI-Dienste uebermittelt?
      3. **Output-Qualitaet**: Wie oft muessen KI-Ergebnisse korrigiert werden?
      4. **Compliance**: Werden interne Richtlinien eingehalten?
      5. **Transparenz**: Wird KI-Einsatz gegenueber dem Publikum offengelegt?
      6. **Datenschutz**: Werden personenbezogene Daten und Quellen geschuetzt?
      7. **Kosteneffizienz**: Stehen Aufwand und Nutzen in sinnvollem Verhaeltnis?

      Ziel ist **nachvollziehbare Qualitaet** -- unabhaengig davon, ob Inhalte von
      Menschen oder mit KI erstellt wurden.
    examples:
      - q: "Ein KI-Audit ergibt, dass 40% der KI-generierten Zusammenfassungen nachkorrigiert werden muessen. Ist das ein akzeptabler Wert?"
        a: "Das haengt vom Vergleichswert ab: Wie hoch ist die Korrekturrate bei menschlich erstellten Zusammenfassungen? Wenn dort nur 10% korrigiert werden, ist 40% klar zu hoch. Wenn der Wert bei 35% liegt, bringt KI kaum Qualitaetsgewinn. Entscheidend ist: Die Fehlerrate muss gemessen, dokumentiert und regelmaessig ueberprueft werden. Erst dann kann man bewerten, ob KI fuer diese Aufgabe sinnvoll ist"
        rel:
          - ["Fehlerrate messen", "Quantitative Qualitaetsbewertung von KI-Output"]

      - q: "Bei einem Datenfluss-Audit stellt sich heraus, dass drei Reporter vertrauliche Interviewnotizen in ChatGPT hochgeladen haben. Was nun?"
        a: "Sofortmassnahme: Pruefen, ob identifizierbare Quellen betroffen sind, und wenn ja, die Quellen informieren. Mittelfristig: Klare Richtlinien aufstellen (oder bestehende durchsetzen), sichere Alternativen bereitstellen und Schulungen durchfuehren. Nicht bestrafen, sondern das Problem systemisch loesen -- fehlende Richtlinien und mangelnde Schulung sind ein Organisationsversagen, kein individuelles"
        rel:
          - ["Datenschutz-Audit", "Reaktion auf Richtlinienverletzungen im KI-Einsatz"]

      - q: "Warum sollten an der Entwicklung von KI-Richtlinien nicht nur IT und Redaktion, sondern auch Audience-Teams beteiligt sein?"
        a: "Weil Audience-Teams die Perspektive der Leser einbringen: Was erwarten die Nutzer an Transparenz? Wie reagieren sie auf KI-Kennzeichnungen? Lehnen sie KI-generierte Inhalte ab? Die Studie der Landesmedienanstalten zeigt, dass 90% der Befragten Kennzeichnung fordern -- dieses Signal muss in die Richtlinien einfliessen. Richtlinien, die nur intern gedacht werden, riskieren am Leser vorbei zu regulieren"
        rel:
          - ["Audience-Perspektive", "Lesererwartungen in KI-Richtlinien einbeziehen"]

      - q: "Eine kleine Redaktion hat keine Ressourcen fuer ein formales KI-Audit. Was ist das absolute Minimum?"
        a: "Drei Fragen monatlich beantworten: 1) Welche KI-Tools nutzen wir und fuer was? (Tool-Inventar als Liste). 2) Wurden sensible Daten in Cloud-KI-Tools eingegeben? (Schneller Datenschutz-Check). 3) Wie oft mussten KI-Ergebnisse korrigiert werden? (Fehlerrate schaetzen). Das dauert 30 Minuten und gibt einen Grundueberblick. Die AP, SPJ und Partnership on AI bieten kostenlose Orientierungshilfen fuer Richtlinien"
        rel:
          - ["Minimal-Audit", "Drei Kernfragen fuer Redaktionen mit begrenzten Ressourcen"]
