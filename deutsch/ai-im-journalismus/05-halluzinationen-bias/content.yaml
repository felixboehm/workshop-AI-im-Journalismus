version: 1
number: 5
title: "Halluzinationen und Bias"
description: "Die groessten Gefahren beim KI-Einsatz im Journalismus"
sections:
  - title: "KI-Halluzinationen"
    explanation: |
      **Halluzinationen** entstehen, wenn KI plausibel klingende, aber faktisch falsche Informationen
      generiert. Das Problem ist gravierend:

      - KI-Chatbots weisen **Halluzinationsraten von bis zu 79%** auf
      - Eine Studie des Tow Center ergab, dass **ueber 60% der Antworten von KI-Suchmaschinen ungenau** waren
      - Halluzinationen umfassen: erfundene Zitate, falsche Statistiken, nicht existierende Quellen

      Besonders gefaehrlich sind Halluzinationen unter Zeitdruck bei Echtzeit-Berichterstattung,
      da die plausibel klingenden Fehler schwer zu erkennen sind.
    examples:
      - q: "Ein Redakteur nutzt KI fuer einen Hintergrundartikel und die KI liefert drei Studien mit DOI-Nummern als Beleg. Wie verlaesslich ist das?"
        a: "Gar nicht -- KI erfindet regelmaessig Quellen inklusive plausibler DOI-Nummern, Autorennamen und Zeitschriftentitel. Das ist eine der tueckischsten Halluzinationsformen, weil sie professionell aussieht. Jede genannte Quelle muss manuell verifiziert werden. Die Tow-Center-Studie zeigt: ueber 60% der KI-Suchmaschinen-Antworten waren ungenau"
        rel:
          - ["Erfundene Quellen", "KI generiert plausible aber fiktive Quellenangaben"]

      - q: "CNET veroeffentlichte 77 KI-Finanzartikel, ueber die Haelfte hatte Fehler. Warum fiel das nicht frueher auf?"
        a: "Weil die Fehler plausibel klangen -- falsche Erklaerungen von Zinseszins oder Sparkonten sind fuer Nicht-Experten schwer zu erkennen. CNET veroeffentlichte unter dem Autorennamen 'CNET Money Staff' ohne ausreichende Fachpruefung. Das zeigt: KI-Output braucht nicht nur Korrekturlesen, sondern inhaltliche Pruefung durch Fachleute"
        rel:
          - ["CNET-Skandal", "Plausibel klingende Fehler erfordern Fachpruefung"]

      - q: "Warum halluziniert KI unter Zeitdruck nicht haeufiger, sondern ist immer gleich unzuverlaessig?"
        a: "Der Zeitdruck liegt nicht bei der KI, sondern beim Journalisten. Die KI halluziniert immer mit der gleichen Rate -- aber unter Zeitdruck faellt die menschliche Pruefung weg. Genau das macht Echtzeit-Berichterstattung so gefaehrlich: Wenn ein Live-Ticker aus KI-Zusammenfassungen gespeist wird und niemand gegenprueft, gelangen erfundene Details unbemerkt zum Leser"
        rel:
          - ["Zeitdruck-Risiko", "Nicht die KI wird schlechter, die menschliche Kontrolle fehlt"]

      - q: "Google AI Overviews empfahl, Steine zu essen und Klebstoff auf Pizza zu tun. Wie konnte das passieren?"
        a: "Die KI uebernahm satirische Beitraege aus Foren als Fakten, weil sie nicht zwischen Satire und ernsthaften Quellen unterscheiden kann. Die Canadian Medical Association bezeichnete KI-generierte Gesundheitsratschlaege als gefaehrlich. Fuer Journalisten bedeutet das: KI hat kein Verstaendnis fuer Kontext, Ironie oder die Qualitaet einer Quelle -- sie imitiert Sprache, ohne den Inhalt zu begreifen"
        rel:
          - ["Kontextblindheit", "KI unterscheidet nicht zwischen Satire und Fakten"]

  - title: "Bias und Verzerrungen"
    explanation: |
      **Bias** (systematische Verzerrung) entsteht durch die Trainingsdaten der KI-Modelle:

      - Trainingsdaten koennen voreingenommen, ungenau oder absichtlich manipuliert sein
      - Staatliche und nichtstaatliche Akteure koennen Narrative in Trainingsdaten einschleusen
      - Systematische Unterrepraesentation bestimmter Perspektiven und Bevoelkerungsgruppen
      - Verstaerkung bestehender gesellschaftlicher Vorurteile

      Fuer den Journalismus bedeutet das: KI kann einseitige Darstellungen produzieren,
      ohne dass dies auf den ersten Blick erkennbar ist.
    examples:
      - q: "Eine Redakteurin laesst KI Hintergrundinformationen zu einem Konflikt im Nahen Osten zusammenstellen. Wie wahrscheinlich ist ein Bias?"
        a: "Sehr wahrscheinlich. KI-Trainingsdaten spiegeln die Verteilung englischsprachiger Online-Inhalte wider -- bestimmte Perspektiven sind massiv ueberrepraesentiert, andere kaum vorhanden. Bei geopolitischen Themen fuehrt das fast zwangslaefig zu einseitigen Darstellungen. Die Redakteurin sollte die KI-Zusammenfassung gezielt mit Quellen aus unterschiedlichen Regionen und Perspektiven abgleichen"
        rel:
          - ["Geopolitischer Bias", "Ueberrepraesentation dominanter Narrative in Trainingsdaten"]

      - q: "Kann KI-Bias auch bei scheinbar neutralen Themen wie Wirtschaftsberichterstattung auftreten?"
        a: "Ja -- wenn die KI z.B. Unternehmensberichte zusammenfasst, koennte sie systematisch positive Aspekte betonen, weil Pressemitteilungen und Unternehmens-PR in den Trainingsdaten ueberrepraesentiert sind. Oder sie koennte bei Arbeitsmarktthemen bestimmte Branchen oder Demografien bevorzugen. Journalisten sollten immer fragen: Welche Perspektive fehlt in diesem KI-Output?"
        rel:
          - ["Unsichtbarer Bias", "Verzerrungen bei scheinbar neutralen Themen"]

      - q: "Die russische Kampagne 'Operation Overload' nutzte KI fuer Desinformation. Koennten aehnliche Akteure KI-Trainingsdaten manipulieren?"
        a: "Ja -- staatliche und nichtstaatliche Akteure koennen Narrative gezielt in Online-Inhalte einschleusen, die dann als Trainingsdaten in KI-Modelle einfliessen. Operation Overload imitierte 2025 ueber 80 Organisationen mit manipulierten Logos und Stimmen. KI-Bias durch Datenmanipulation ist schwerer zu erkennen als offensichtliche Desinformation, weil er subtil in die Grundannahmen der KI eingebaut ist"
        rel:
          - ["Gezielte Datenmanipulation", "Staatliche Akteure und KI-Trainingsdaten"]

      - q: "Wie kann eine Redaktion systematisch pruefen, ob ein KI-generierter Text verzerrt ist?"
        a: "Dreistufig: Erstens, den Output mit Quellen aus unterschiedlichen Perspektiven vergleichen. Zweitens, gezielt nach fehlenden Stimmen fragen ('Welche Betroffenengruppe kommt nicht vor?'). Drittens, denselben Prompt an verschiedene KI-Modelle geben und Abweichungen analysieren. Wenn Claude und ChatGPT das gleiche Thema unterschiedlich gewichten, ist das ein Signal fuer Bias in einem der Modelle"
        rel:
          - ["Bias-Pruefung", "Systematische Verzerrungsanalyse in drei Schritten"]

  - title: "Fallbeispiele: Wenn KI schiefgeht"
    explanation: |
      Reale Faelle zeigen die Risiken:

      - **Sports Illustrated (2023)**: Artikel unter fiktiven Autorennamen mit KI-generierten Biografien -- erfundene Teammitglieder wie "Drew Ortiz" und "Sora Tanaka"
      - **Gannett/USA Today**: KI-generierte Sportberichte mit absurden Phrasen wie "close encounter of the athletic kind"
      - **Google AI Overviews (2024)**: Empfahl Steine zu essen und Klebstoff auf Pizza zu tun (uebernahm satirische Forum-Beitraege als Fakten)
      - **Perplexity AI (2024)**: Systematisches Plagiat von Paywall-geschuetzten Exklusivinhalten von Forbes und Wired

      Alle Faelle zeigen: Ohne menschliche Kontrolle entstehen Fehler, die das Vertrauen der Leser zerstoeren.
    examples:
      - q: "Sports Illustrated erfand Autoren wie 'Drew Ortiz' mit KI-generierten Fotos und Biografien. Wem schadet das am meisten?"
        a: "Nicht nur Sports Illustrated, sondern dem gesamten Journalismus. Wenn Leser erfahren, dass Autoren erfunden sein koennen, sinkt das Vertrauen in alle Bylines. Gleichzeitig wurden die KI-generierten Kaufratgeber ueber Affiliate-Links monetarisiert -- das ist Taeuschung der Leser fuer finanziellen Gewinn. Der Skandal fuehrte zu Entlassungen und schwerer Reputationsschaedigung"
        rel:
          - ["Vertrauensverlust", "Einzelne Skandale schaedigen den gesamten Journalismus"]

      - q: "Gannett/USA Today veroeffentlichte 'grotesk verstuemmelte' KI-Sportberichte. Warum sind automatisierte Sportberichte bei RADAR erfolgreich, bei Gannett aber nicht?"
        a: "Der entscheidende Unterschied: Bei RADAR schreiben Journalisten jeden Satz als Template, die Software lokalisiert nur mit Daten. Bei Gannett/LedeAI generierte die KI frei Text -- mit absurden Ergebnissen wie 'close encounter of the athletic kind'. Der Fehler lag im Workflow-Design: Freitext-Generierung statt Template-basierter Lokalisierung, und fehlende menschliche Pruefung vor Veroeffentlichung"
        rel:
          - ["Template vs. Freitext", "RADAR-Modell vs. Gannett-Ansatz bei Automatisierung"]

      - q: "Perplexity AI plagiierte Paywall-geschuetzte Forbes-Inhalte und nutzte gefaelschte User-Agent-Strings zum Scraping. Was bedeutet das fuer Redaktionen, die Perplexity zur Recherche nutzen?"
        a: "Zwei Risiken: Erstens, die Quellen in Perplexity koennten auf gestohlenem Material basieren -- ethisch problematisch fuer Journalisten, die auf saubere Quellenarbeit angewiesen sind. Zweitens, Perplexity reproduziert Inhalte mit minimaler Quellenangabe. Die NYT hat Perplexity deshalb verklagt. Fuer die Recherche ist Perplexity nuetzlich, aber die Originalquelle muss immer direkt verifiziert werden"
        rel:
          - ["Plagiat-Risiko", "Ethische Bedenken bei der Nutzung von KI-Recherchetools"]

      - q: "Alle diese Faelle haben eines gemeinsam. Was ist die uebergreifende Lehre?"
        a: "In jedem Fall fehlte die menschliche Kontrolle an der entscheidenden Stelle: CNET pruefte Inhalte nicht fachlich, Sports Illustrated pruefte Autorenidentitaeten nicht, Gannett veroeffentlichte ohne Redaktionspruefung, Google filterte nicht zwischen Satire und Fakten. Die Technologie hat nicht versagt -- die Prozesse haben versagt. Human-in-the-Loop ist kein Nice-to-have, sondern die Minimalanforderung"
        rel:
          - ["Prozessversagen", "Nicht die Technologie, sondern fehlende Kontrolle war das Problem"]
